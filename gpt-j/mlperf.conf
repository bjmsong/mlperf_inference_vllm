# from https://github.com/bjmsong/inference_results_v3.1/blob/main/closed/Dell/code/gptj-99/pytorch-cpu/mlperf.conf
# The format of this config file is 'key = value'.
# The key has the format 'model.scenario.key'. Value is mostly int64_t.
# Model maybe '*' as wildcard. In that case the value applies to all models.
# All times are in milli seconds

# Set performance_sample_count for each model.
# User can optionally set this to higher values in user.conf.
gptj.*.performance_sample_count_override = 1000


# Set seeds. The seeds will be distributed two weeks before the submission.
*.*.qsl_rng_seed = 13281865557512327830
*.*.sample_index_rng_seed = 198141574272810017
*.*.schedule_rng_seed = 7575108116881280410
# Set seeds for TEST_05. The seeds will be distributed two weeks before the submission.
*.*.test05_qsl_rng_seed = 2376919268182438552
*.*.test05_sample_index_rng_seed = 11176391829184272374
*.*.test05_schedule_rng_seed = 3911940905271271337


*.SingleStream.target_latency_percentile = 90
*.SingleStream.min_duration = 600000

*.MultiStream.target_latency_percentile = 99
*.MultiStream.samples_per_query = 8
*.MultiStream.min_duration = 600000
*.MultiStream.min_query_count = 662
retinanet.MultiStream.target_latency = 528

# LLM benchmarks have non-uniform inputs and outputs, and use equal issue mode for all latency scenario
gptj.*.sample_concatenate_permutation = 1
llama2-70b.*.sample_concatenate_permutation = 1

*.Server.target_latency = 10
*.Server.target_latency_percentile = 99
*.Server.target_duration = 0
*.Server.min_duration = 600000
bert.Server.target_latency = 130
gptj.Server.target_latency = 20000
stable-diffusion-xl.Server.target_latency = 20000
# Llama2-70b benchmarks measures token latencies
llama2-70b.*.use_token_latencies = 1
# gptj benchmark infers token latencies
gptj.*.infer_token_latencies = 1
gptj.*.token_latency_scaling_factor = 69
# Only ttft and tpot are tracked for the llama2-70b benchmark therefore target_latency = 0
llama2-70b.Server.target_latency = 0
llama2-70b.Server.ttft_latency = 2000
llama2-70b.Server.tpot_latency = 200

*.Offline.target_latency_percentile = 90
*.Offline.min_duration = 600000

# In Offline scenario, we always have one query. But LoadGen maps this to
# min_sample_count internally in Offline scenario. If the dataset size is larger 
# than 24576 we limit the min_query_count to 24576 and otherwise we use 
# the dataset size as the limit

bert.Offline.min_query_count = 10833
gptj.Offline.min_query_count = 1000
stable-diffusion-xl.Offline.min_query_count = 5000
llama2-70b.Offline.min_query_count = 24576

# These fields should be defined and overridden by user.conf.
*.SingleStream.target_latency = 10
*.MultiStream.target_latency = 80
*.Server.target_qps = 1.0
*.Offline.target_qps = 1.0
